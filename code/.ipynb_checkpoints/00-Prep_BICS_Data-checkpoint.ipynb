{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0c3bdef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "da750851",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/chrissoria/Documents/Research/BICS_Political_Polarization/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5a7d4",
   "metadata": {},
   "source": [
    "some ZIP Codes, such as \"unique\" ZIPs that are often tied to an internal mail-routing mechanism (such as UC Berkeley which has its own internal ZIP code and mail-routing system) will have very low ratios). \\\n",
    "As of 12/23 I'm realzing that I'll need to start with a complete list of all ZIP codes (from USPS) before I start this so I can get a good sense of completeness. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f9750",
   "metadata": {},
   "source": [
    "Below, I'm reading in that full list of ZIP codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "02309edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_to_CD_df = pd.read_excel('Data/ZIP_CD_062020.xlsx')\n",
    "\n",
    "ZIP_to_CD_df = ZIP_to_CD_df.rename(columns={\n",
    "    'RES_RATIO': 'CD_RES_RATIO',\n",
    "    'BUS_RATIO': 'CD_BUS_RATIO',\n",
    "    'OTH_RATIO': 'CD_OTH_RATIO',\n",
    "    'TOT_RATIO': 'CD_TOT_RATIO'\n",
    "})\n",
    "\n",
    "ZIP_to_CD_df['ZIP'] = ZIP_to_CD_df['ZIP'].astype(str).str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "faf9b462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49649\n",
      "37714\n",
      "     ZIP USPS_STATE             TYPE\n",
      "0  01001         MA  Standard/PO BOX\n",
      "1  01002         MA  Standard/PO BOX\n",
      "2  01004         MA  Standard/PO BOX\n",
      "3  01005         MA  Standard/PO BOX\n",
      "4  01007         MA  Standard/PO BOX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrissoria/anaconda3/lib/python3.11/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    }
   ],
   "source": [
    "col_types = {\n",
    "    'DELIVERY ZIPCODE': str\n",
    "}\n",
    "\n",
    "FULL_ZIPS = pd.read_excel('Data/Full_Zips.xlsx', dtype=col_types)\n",
    "\n",
    "print(len(FULL_ZIPS))\n",
    "\n",
    "FULL_ZIPS = FULL_ZIPS[['DELIVERY ZIPCODE','PHYSICAL STATE','TYPE']]\n",
    "\n",
    "FULL_ZIPS = FULL_ZIPS.rename(columns={\n",
    "    'DELIVERY ZIPCODE': 'ZIP',\n",
    "    'PHYSICAL STATE': 'USPS_STATE'\n",
    "})\n",
    "\n",
    "#removing ZIP codes that aren't in US main\n",
    "FULL_ZIPS = FULL_ZIPS.drop_duplicates(subset='ZIP', keep='first')\n",
    "FULL_ZIPS['ZIP'] = FULL_ZIPS['ZIP'].astype(str).str.zfill(5)\n",
    "FULL_ZIPS = FULL_ZIPS[pd.notna(FULL_ZIPS['USPS_STATE'])].reset_index(drop=True)\n",
    "exclude_states = [\"VI\", \"PR\", \"MH\", \"MP\", \"FM\", \"GU\"]\n",
    "FULL_ZIPS = FULL_ZIPS[~FULL_ZIPS['USPS_STATE'].isin(exclude_states)].reset_index(drop=True)\n",
    "FULL_ZIPS = FULL_ZIPS[~FULL_ZIPS['TYPE'].isin([\"Other\", \"Unique\"])].reset_index(drop=True)\n",
    "\n",
    "print(len(FULL_ZIPS))\n",
    "print(FULL_ZIPS.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ffa5e",
   "metadata": {},
   "source": [
    "After merging, I find that many of the Alaska ZIP codes aren't getting matched. Since there's only one CD in Alaska, I'll fill in any Alaska state ZIP with 0200. \\\n",
    "This data will not match PO boxes. Most non matches are coming from PO boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "06e1d800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZIP</th>\n",
       "      <th>USPS_STATE</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>CD</th>\n",
       "      <th>CD_RES_RATIO</th>\n",
       "      <th>CD_BUS_RATIO</th>\n",
       "      <th>CD_OTH_RATIO</th>\n",
       "      <th>CD_TOT_RATIO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01001</td>\n",
       "      <td>MA</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>2501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01002</td>\n",
       "      <td>MA</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>2502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01004</td>\n",
       "      <td>MA</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>2502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01005</td>\n",
       "      <td>MA</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>2502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01007</td>\n",
       "      <td>MA</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>2502</td>\n",
       "      <td>0.998118</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44276</th>\n",
       "      <td>99925</td>\n",
       "      <td>AK</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>0200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44277</th>\n",
       "      <td>99926</td>\n",
       "      <td>AK</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>0200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44278</th>\n",
       "      <td>99927</td>\n",
       "      <td>AK</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>0200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44279</th>\n",
       "      <td>99928</td>\n",
       "      <td>AK</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>0200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44280</th>\n",
       "      <td>99929</td>\n",
       "      <td>AK</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>0200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44281 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ZIP USPS_STATE             TYPE    CD  CD_RES_RATIO  CD_BUS_RATIO  \\\n",
       "0      01001         MA  Standard/PO BOX  2501      1.000000           1.0   \n",
       "1      01002         MA  Standard/PO BOX  2502      1.000000           1.0   \n",
       "2      01004         MA  Standard/PO BOX  2502      1.000000           1.0   \n",
       "3      01005         MA  Standard/PO BOX  2502      1.000000           1.0   \n",
       "4      01007         MA  Standard/PO BOX  2502      0.998118           1.0   \n",
       "...      ...        ...              ...   ...           ...           ...   \n",
       "44276  99925         AK  Standard/PO BOX  0200      0.000000           0.0   \n",
       "44277  99926         AK  Standard/PO BOX  0200      0.000000           0.0   \n",
       "44278  99927         AK  Standard/PO BOX  0200      0.000000           0.0   \n",
       "44279  99928         AK  Standard/PO BOX  0200      0.000000           0.0   \n",
       "44280  99929         AK  Standard/PO BOX  0200      0.000000           0.0   \n",
       "\n",
       "       CD_OTH_RATIO  CD_TOT_RATIO  \n",
       "0               1.0      1.000000  \n",
       "1               1.0      1.000000  \n",
       "2               1.0      1.000000  \n",
       "3               1.0      1.000000  \n",
       "4               1.0      0.998217  \n",
       "...             ...           ...  \n",
       "44276           1.0      1.000000  \n",
       "44277           1.0      1.000000  \n",
       "44278           1.0      1.000000  \n",
       "44279           1.0      1.000000  \n",
       "44280           1.0      1.000000  \n",
       "\n",
       "[44281 rows x 8 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ZIP_FEATURES = pd.merge(FULL_ZIPS, ZIP_to_CD_df, on = \"ZIP\", how = \"left\") #merging to full zips here\n",
    "\n",
    "ZIP_FEATURES['CD'] = ZIP_FEATURES.apply(lambda row: \"0200\" if row['USPS_STATE'] == \"AK\" else row['CD'], axis = 1)\n",
    "ZIP_FEATURES['CD_TOT_RATIO'] = ZIP_FEATURES.apply(lambda row: 1.0 if row['USPS_STATE'] == \"AK\" else row['CD_TOT_RATIO'], axis = 1)\n",
    "\n",
    "ZIP_FEATURES['CD'] = ZIP_FEATURES.apply(lambda row: \"4212\" if row['ZIP'] == \"15253\" else row['CD'],\n",
    "                                       axis=1)\n",
    "\n",
    "ZIP_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed36d26",
   "metadata": {},
   "source": [
    "For now, I will assign a ZIP code to whatever whatever County it falls mostly under residentially \\\n",
    "This will NOT include DC simply because DC doesn't have a CD \\\n",
    "IN A FUTURE VERSION, I WILL HAVE TO START WITH A LIST OF ZIP CODES THAT INCLUDE DISTRICT OF COLUMBIA'S (WHICH I  HAVE ABOVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c89f7a",
   "metadata": {},
   "source": [
    "I'm running this as an outer match because then I can keep zip codes from the right side that aren't in the left side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "61dc7c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ZIP COUNTY  RES_RATIO  BUS_RATIO  OTH_RATIO  TOT_RATIO\n",
      "0  00501  36103   0.000000   1.000000   0.000000   1.000000\n",
      "1  00601  72113   0.160758   0.199017   0.128834   0.162397\n",
      "2  00601  72001   0.839242   0.800983   0.871166   0.837603\n",
      "3  00602  72003   1.000000   0.998801   1.000000   0.999919\n",
      "4  00602  72005   0.000000   0.001199   0.000000   0.000081\n",
      "         ZIP USPS_STATE             TYPE    CD  CD_RES_RATIO  CD_BUS_RATIO  \\\n",
      "0      01001         MA  Standard/PO BOX  2501           1.0           1.0   \n",
      "28914  45729         OH  Standard/PO BOX  3906           1.0           1.0   \n",
      "28955  45771         OH  Standard/PO BOX  3906           1.0           1.0   \n",
      "28954  45770         OH  Standard/PO BOX  3906           1.0           1.0   \n",
      "28953  45769         OH  Standard/PO BOX  3906           1.0           1.0   \n",
      "\n",
      "       CD_OTH_RATIO  CD_TOT_RATIO COUNTY  RES_RATIO  BUS_RATIO  OTH_RATIO  \\\n",
      "0               1.0           1.0  25013        1.0        1.0        1.0   \n",
      "28914           1.0           1.0  39167        1.0        1.0        1.0   \n",
      "28955           1.0           1.0  39105        1.0        1.0        1.0   \n",
      "28954           0.0           1.0  39105        1.0        1.0        0.0   \n",
      "28953           1.0           1.0  39105        1.0        1.0        1.0   \n",
      "\n",
      "       TOT_RATIO  \n",
      "0            1.0  \n",
      "28914        1.0  \n",
      "28955        1.0  \n",
      "28954        1.0  \n",
      "28953        1.0  \n",
      "464\n",
      "1.1622955336790162\n"
     ]
    }
   ],
   "source": [
    "col_types = {\n",
    "    'COUNTY': str,\n",
    "    'ZIP': str\n",
    "}\n",
    "\n",
    "ZIP_to_County_df = pd.read_excel('Data/ZIP_COUNTY_062020_HUD.xlsx', dtype=col_types)\n",
    "ZIP_to_County_df['COUNTY'] = ZIP_to_County_df['COUNTY'].astype(str).str.zfill(5)\n",
    "ZIP_to_County_df['ZIP'] = ZIP_to_County_df['ZIP'].astype(str).str.zfill(5)\n",
    "print(ZIP_to_County_df.head())\n",
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, ZIP_to_County_df, how ='outer', on = \"ZIP\")\n",
    "\n",
    "# Sort the DataFrame by RES_RATIO in descending order\n",
    "ZIP_FEATURES = ZIP_FEATURES.sort_values(by='RES_RATIO', ascending=False)\n",
    "ZIP_FEATURES = ZIP_FEATURES.drop_duplicates(subset='ZIP', keep='first')\n",
    "\n",
    "ZIP_to_County_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ZIP_to_County_df = ZIP_to_County_df.rename(columns={\n",
    "    'RES_RATIO': 'COUNTY_RES_RATIO',\n",
    "    'BUS_RATIO': 'COUNTY_BUS_RATIO',\n",
    "    'OTH_RATIO': 'COUNTY_OTH_RATIO',\n",
    "    'TOT_RATIO': 'COUNTY_TOT_RATIO'\n",
    "})\n",
    "\n",
    "print(ZIP_FEATURES.head())\n",
    "\n",
    "print(ZIP_FEATURES['COUNTY'].isna().sum()) #high level of match\n",
    "print(ZIP_FEATURES['COUNTY'].isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8685d",
   "metadata": {},
   "source": [
    "I'm realizing I'll have to merge the school district data onto ZIP_to_County_df (give that there's some zip codes that don't have a school district associated) \\\n",
    "Yet, the issue of some ZIP codes falling through the cracks will still be there. \\\n",
    "I'll need something that goes from county and matches to all ZIP codes within OR \\\n",
    "Something that matches ZIP codes directly to the nearest school district (IDEAL). \\\n",
    "For now (12/26/23) this variable is not complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b889c9",
   "metadata": {},
   "source": [
    "Instead, I'll match the ZIP code to a census tract THEN match tract onto school district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ef1eb603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZIP</th>\n",
       "      <th>USPS_STATE</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>CD</th>\n",
       "      <th>CD_RES_RATIO</th>\n",
       "      <th>CD_BUS_RATIO</th>\n",
       "      <th>CD_OTH_RATIO</th>\n",
       "      <th>CD_TOT_RATIO</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>RES_RATIO_x</th>\n",
       "      <th>BUS_RATIO_x</th>\n",
       "      <th>OTH_RATIO_x</th>\n",
       "      <th>TOT_RATIO_x</th>\n",
       "      <th>TRACT</th>\n",
       "      <th>RES_RATIO_y</th>\n",
       "      <th>BUS_RATIO_y</th>\n",
       "      <th>OTH_RATIO_y</th>\n",
       "      <th>TOT_RATIO_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01001</td>\n",
       "      <td>MA</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>2501</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25013813209</td>\n",
       "      <td>0.404431</td>\n",
       "      <td>0.442809</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.403828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45729</td>\n",
       "      <td>OH</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>3906</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39167021700</td>\n",
       "      <td>0.752549</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.749772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45771</td>\n",
       "      <td>OH</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>3906</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39105964600</td>\n",
       "      <td>0.793269</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.788959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45770</td>\n",
       "      <td>OH</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>3906</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39105964600</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.924791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45769</td>\n",
       "      <td>OH</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>3906</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39105964200</td>\n",
       "      <td>0.475592</td>\n",
       "      <td>0.139276</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.440522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39916</th>\n",
       "      <td>99711</td>\n",
       "      <td>AK</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>0200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39917</th>\n",
       "      <td>99716</td>\n",
       "      <td>AK</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>0200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39918</th>\n",
       "      <td>99802</td>\n",
       "      <td>AK</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>0200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39919</th>\n",
       "      <td>99803</td>\n",
       "      <td>AK</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>0200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39920</th>\n",
       "      <td>99821</td>\n",
       "      <td>AK</td>\n",
       "      <td>Standard/PO BOX</td>\n",
       "      <td>0200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39921 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ZIP USPS_STATE             TYPE    CD  CD_RES_RATIO  CD_BUS_RATIO  \\\n",
       "0      01001         MA  Standard/PO BOX  2501           1.0           1.0   \n",
       "1      45729         OH  Standard/PO BOX  3906           1.0           1.0   \n",
       "2      45771         OH  Standard/PO BOX  3906           1.0           1.0   \n",
       "3      45770         OH  Standard/PO BOX  3906           1.0           1.0   \n",
       "4      45769         OH  Standard/PO BOX  3906           1.0           1.0   \n",
       "...      ...        ...              ...   ...           ...           ...   \n",
       "39916  99711         AK  Standard/PO BOX  0200           NaN           NaN   \n",
       "39917  99716         AK  Standard/PO BOX  0200           NaN           NaN   \n",
       "39918  99802         AK  Standard/PO BOX  0200           NaN           NaN   \n",
       "39919  99803         AK  Standard/PO BOX  0200           NaN           NaN   \n",
       "39920  99821         AK  Standard/PO BOX  0200           NaN           NaN   \n",
       "\n",
       "       CD_OTH_RATIO  CD_TOT_RATIO COUNTY  RES_RATIO_x  BUS_RATIO_x  \\\n",
       "0               1.0           1.0  25013          1.0          1.0   \n",
       "1               1.0           1.0  39167          1.0          1.0   \n",
       "2               1.0           1.0  39105          1.0          1.0   \n",
       "3               0.0           1.0  39105          1.0          1.0   \n",
       "4               1.0           1.0  39105          1.0          1.0   \n",
       "...             ...           ...    ...          ...          ...   \n",
       "39916           NaN           1.0    NaN          NaN          NaN   \n",
       "39917           NaN           1.0    NaN          NaN          NaN   \n",
       "39918           NaN           1.0    NaN          NaN          NaN   \n",
       "39919           NaN           1.0    NaN          NaN          NaN   \n",
       "39920           NaN           1.0    NaN          NaN          NaN   \n",
       "\n",
       "       OTH_RATIO_x  TOT_RATIO_x        TRACT  RES_RATIO_y  BUS_RATIO_y  \\\n",
       "0              1.0          1.0  25013813209     0.404431     0.442809   \n",
       "1              1.0          1.0  39167021700     0.752549     0.533333   \n",
       "2              1.0          1.0  39105964600     0.793269     0.693548   \n",
       "3              0.0          1.0  39105964600     0.923077     1.000000   \n",
       "4              1.0          1.0  39105964200     0.475592     0.139276   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "39916          NaN          NaN          NaN          NaN          NaN   \n",
       "39917          NaN          NaN          NaN          NaN          NaN   \n",
       "39918          NaN          NaN          NaN          NaN          NaN   \n",
       "39919          NaN          NaN          NaN          NaN          NaN   \n",
       "39920          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "       OTH_RATIO_y  TOT_RATIO_y  \n",
       "0         0.250000     0.403828  \n",
       "1         1.000000     0.749772  \n",
       "2         0.692308     0.788959  \n",
       "3         0.000000     0.924791  \n",
       "4         0.127660     0.440522  \n",
       "...            ...          ...  \n",
       "39916          NaN          NaN  \n",
       "39917          NaN          NaN  \n",
       "39918          NaN          NaN  \n",
       "39919          NaN          NaN  \n",
       "39920          NaN          NaN  \n",
       "\n",
       "[39921 rows x 18 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_types = {\n",
    "    'TRACT': str,\n",
    "    'ZIP': str\n",
    "}\n",
    "\n",
    "ZIP_to_Tract = pd.read_excel('data/ZIP_TRACT_062020.xlsx', dtype=col_types)\n",
    "\n",
    "ZIP_to_Tract = ZIP_to_Tract.sort_values(by='RES_RATIO', ascending=False)\n",
    "ZIP_to_Tract = ZIP_to_Tract.drop_duplicates(subset='ZIP', keep='first')\n",
    "\n",
    "ZIP_to_Tract.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, ZIP_to_Tract, on = \"ZIP\", how = \"left\")\n",
    "\n",
    "ZIP_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab79ef9",
   "metadata": {},
   "source": [
    "Now, let's match this tract to a school district. \\\n",
    "After merging this file, I find that there are still a large chunk of rows without a school district \\\n",
    "For these rows, I'll use a cruder approximation of school district at the county-level (which might be too broad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eaa0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {\n",
    "    'LEAID': str,\n",
    "    'TRACT': str\n",
    "}\n",
    "\n",
    "school_to_tract = pd.read_excel('data/grf20_lea_tract.xlsx', dtype=col_types)\n",
    "\n",
    "\n",
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, school_to_tract, on = \"TRACT\", how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a55d4d0",
   "metadata": {},
   "source": [
    "Below, I'll read in the county-school district level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58dd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {\n",
    "    'LEAID': str,\n",
    "    'STCOUNTY': str\n",
    "}\n",
    "\n",
    "County_to_District = pd.read_excel('data/grf20_lea_county.xlsx', dtype=col_types)\n",
    "County_to_District = County_to_District.sort_values(by='LANDAREA', ascending=False)\n",
    "County_to_District = County_to_District.drop_duplicates(subset='STCOUNTY', keep='first')\n",
    "\n",
    "County_to_District = County_to_District.rename(columns={\"STCOUNTY\": \"COUNTY\"})\n",
    "\n",
    "County_to_District = County_to_District[['NAME_LEA20','COUNTY','LEAID']]\n",
    "\n",
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, County_to_District, on = \"COUNTY\", how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09519af",
   "metadata": {},
   "source": [
    "We also want to add data on mask mandates at the county-level in order to catpure \"given that a mask mandatte was or wasn't implementd...partisans contact is...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3119cf",
   "metadata": {},
   "source": [
    "Next, I'll clean up the data so that we keep only the desired columns, differentiate between the census tract (higher resolution) school district match and the county-level (lower resolution) match. Lastly, I'll create one column for school district based on both of these types of matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c318618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ZIP_FEATURES.head()\n",
    "\n",
    "columns_to_keep = ['ZIP','USPS_STATE','CD','COUNTY','TRACT','LEAID_x','LEAID_y','NAME_LEA20_x','NAME_LEA20_y']\n",
    "\n",
    "ZIP_FEATURES = ZIP_FEATURES[columns_to_keep]\n",
    "\n",
    "ZIP_FEATURES = ZIP_FEATURES.rename(columns = {\n",
    "                                   'NAME_LEA20_x': 'TRACT_SCH_DISTRICT',\n",
    "                                   'NAME_LEA20_y': 'COUNTY_SCH_DISTRICT',\n",
    "                                   'LEAID_x': 'TRACT_LEAID',\n",
    "                                   'LEAID_y': 'COUNTY_LEAID'})\n",
    "\n",
    "ZIP_FEATURES['SCHOOL_DISTRICT'] = ZIP_FEATURES.apply(lambda row: row['COUNTY_SCH_DISTRICT'] if pd.isna(row['TRACT_SCH_DISTRICT']) else row['TRACT_SCH_DISTRICT'], axis = 1)\n",
    "ZIP_FEATURES['LEAID'] = ZIP_FEATURES.apply(lambda row: row['COUNTY_LEAID'] if pd.isna(row['TRACT_LEAID']) else row['TRACT_LEAID'], axis = 1)\n",
    "\n",
    "\n",
    "ZIP_FEATURES.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec3a7ff",
   "metadata": {},
   "source": [
    "NOTE: The school closure data provides best coverage for months after september 2020 to May 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_closures = pd.read_csv('data/District_Monthly_Shares_03.08.23.csv',dtype=str)\n",
    "\n",
    "columns_to_keep = ['NCESDistrictID','month','share_inperson','DistrictName']\n",
    "\n",
    "schools_closures = schools_closures[columns_to_keep]\n",
    "\n",
    "schools_closures['LEAID'] = schools_closures['NCESDistrictID'].apply(lambda x: '00' + x if len(x) == 5 else x)\n",
    "\n",
    "schools_closures['NCESDistrictID'] = schools_closures['LEAID'].apply(lambda x: '0' + x if len(x) == 6 else x)\n",
    "\n",
    "#for mid 2020 (taking two months in case the first is missing August and September)\n",
    "\n",
    "schools_closures_m2020 = schools_closures[schools_closures['month'] == \"2020m8\"].reset_index(drop=True)\n",
    "\n",
    "schools_closures_m2020 = schools_closures_m2020.rename(columns={\n",
    "    'share_inperson': 'august20_school_share_inperson'\n",
    "})\n",
    "\n",
    "schools_closures_m2020 = schools_closures_m2020[['LEAID','august20_school_share_inperson','DistrictName']]\n",
    "\n",
    "#now mnth 9\n",
    "schools_closures_m2020_2 = schools_closures[schools_closures['month'] == \"2020m9\"].reset_index(drop=True)\n",
    "\n",
    "schools_closures_m2020_2 = schools_closures_m2020_2.rename(columns={\n",
    "    'share_inperson': 'september20_school_share_inperson'\n",
    "})\n",
    "\n",
    "schools_closures_m2020_2 = schools_closures_m2020_2[['LEAID','september20_school_share_inperson']]\n",
    "\n",
    "#for month 10 (aggressive capture)\n",
    "\n",
    "schools_closures_m2020_3 = schools_closures[schools_closures['month'] == \"2020m10\"].reset_index(drop=True)\n",
    "\n",
    "schools_closures_m2020_3 = schools_closures_m2020_3.rename(columns={\n",
    "    'share_inperson': 'october20_school_share_inperson'\n",
    "})\n",
    "\n",
    "schools_closures_m2020_3 = schools_closures_m2020_3[['LEAID','october20_school_share_inperson']]\n",
    "\n",
    "#for late 2020\n",
    "\n",
    "schools_closures_l2020 = schools_closures[schools_closures['month'] == \"2020m12\"].reset_index(drop=True)\n",
    "\n",
    "schools_closures_l2020 = schools_closures_l2020.rename(columns={\n",
    "    'share_inperson': 'december20_school_share_inperson'\n",
    "})\n",
    "\n",
    "schools_closures_l2020 = schools_closures_l2020[['LEAID','december20_school_share_inperson']]\n",
    "\n",
    "\n",
    "schools_closures_l2020_2 = schools_closures[schools_closures['month'] == \"2020m11\"].reset_index(drop=True)\n",
    "\n",
    "schools_closures_l2020_2 = schools_closures_l2020_2.rename(columns={\n",
    "    'share_inperson': 'november20_school_share_inperson'\n",
    "})\n",
    "\n",
    "schools_closures_l2020_2 = schools_closures_l2020_2[['LEAID','november20_school_share_inperson']]\n",
    "\n",
    "\n",
    "#for mid 2021\n",
    "\n",
    "schools_closures_m2021 = schools_closures[schools_closures['month'] == \"2021m3\"].reset_index(drop=True)\n",
    "\n",
    "schools_closures_m2021 = schools_closures_m2021.rename(columns={\n",
    "    'share_inperson': 'march21_school_share_inperson'\n",
    "})\n",
    "\n",
    "schools_closures_m2021 = schools_closures_m2021[['LEAID','march21_school_share_inperson']]\n",
    "\n",
    "\n",
    "schools_closures_m2021_2 = schools_closures[schools_closures['month'] == \"2021m2\"].reset_index(drop=True)\n",
    "\n",
    "schools_closures_m2021_2 = schools_closures_m2021_2.rename(columns={\n",
    "    'share_inperson': 'feb21_school_share_inperson'\n",
    "})\n",
    "\n",
    "schools_closures_m2021_2 = schools_closures_m2021_2[['LEAID','feb21_school_share_inperson']]\n",
    "\n",
    "#later mid 2021\n",
    "\n",
    "schools_closures_lm2021 = schools_closures[schools_closures['month'] == \"2021m4\"].reset_index(drop=True)\n",
    "\n",
    "schools_closures_lm2021 = schools_closures_lm2021.rename(columns={\n",
    "    'share_inperson': 'april21_school_share_inperson'\n",
    "})\n",
    "\n",
    "schools_closures_lm2021 = schools_closures_lm2021[['LEAID','april21_school_share_inperson']]\n",
    "\n",
    "\n",
    "schools_closures_lm2021_2 = schools_closures[schools_closures['month'] == \"2021m5\"].reset_index(drop=True)\n",
    "\n",
    "schools_closures_lm2021_2 = schools_closures_lm2021_2.rename(columns={\n",
    "    'share_inperson': 'may21_school_share_inperson'\n",
    "})\n",
    "\n",
    "schools_closures_lm2021_2 = schools_closures_lm2021_2[['LEAID','may21_school_share_inperson']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeddc33",
   "metadata": {},
   "source": [
    "Early fall = Aug-Oct\n",
    "Fall 2020 = Nov-Dec\n",
    "Spring 2021 = Feb-Mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c14d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_closures = schools_closures[~schools_closures['LEAID'].duplicated()]\n",
    "\n",
    "schools_closures = pd.merge(schools_closures, schools_closures_m2020, how = 'left', on = 'LEAID')\n",
    "schools_closures = pd.merge(schools_closures, schools_closures_m2020_2, how = 'left', on = 'LEAID')\n",
    "schools_closures = pd.merge(schools_closures, schools_closures_m2020_3, how = 'left', on = 'LEAID')\n",
    "schools_closures = pd.merge(schools_closures, schools_closures_l2020_2, how = 'left', on = 'LEAID')\n",
    "schools_closures = pd.merge(schools_closures, schools_closures_l2020, how = 'left', on = 'LEAID')\n",
    "schools_closures = pd.merge(schools_closures, schools_closures_m2021_2, how = 'left', on = 'LEAID')\n",
    "schools_closures = pd.merge(schools_closures, schools_closures_m2021, how = 'left', on = 'LEAID')\n",
    "schools_closures = pd.merge(schools_closures, schools_closures_lm2021_2, how = 'left', on = 'LEAID')\n",
    "schools_closures = pd.merge(schools_closures, schools_closures_lm2021, how = 'left', on = 'LEAID')\n",
    "\n",
    "#aug, sept, oct (2020)\n",
    "schools_closures['earlyfall2020_school_closures'] = schools_closures.apply(\n",
    "    lambda row: row['september20_school_share_inperson'] if pd.isna(row['august20_school_share_inperson']) else row['august20_school_share_inperson'], \n",
    "    axis=1\n",
    ")\n",
    "schools_closures['earlyfall2020_school_closures'] = schools_closures.apply(\n",
    "    lambda row: row['october20_school_share_inperson'] if pd.isna(row['earlyfall2020_school_closures']) else row['earlyfall2020_school_closures'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#nov, dec (2020)\n",
    "schools_closures['fall2020_school_closures'] = schools_closures.apply(\n",
    "    lambda row: row['november20_school_share_inperson'] if pd.isna(row['december20_school_share_inperson']) else row['december20_school_share_inperson'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#feb, march (2021)\n",
    "schools_closures['earlyspring2021_school_closures'] = schools_closures.apply(\n",
    "    lambda row: row['feb21_school_share_inperson'] if pd.isna(row['march21_school_share_inperson']) else row['march21_school_share_inperson'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#april, may (2021)\n",
    "\n",
    "schools_closures['spring2021_school_closures'] = schools_closures.apply(\n",
    "    lambda row: row['april21_school_share_inperson'] if pd.isna(row['may21_school_share_inperson']) else row['may21_school_share_inperson'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "schools_closures = schools_closures[['LEAID','earlyfall2020_school_closures','fall2020_school_closures',\n",
    "                                     'earlyspring2021_school_closures','spring2021_school_closures',]]\n",
    "schools_closures.to_csv('data/school_closure_merged.csv')\n",
    "\n",
    "schools_closures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb6e78",
   "metadata": {},
   "source": [
    "let's take a look at coverage. Based on these missing counts, it looks like the best coverage available is for fall 2020, with early spring 2021 and late spring 2021 having a higher percentage of missingness (almost 10 percent of rows contain missing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b45a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts = schools_closures[['LEAID', 'earlyfall2020_school_closures', 'fall2020_school_closures',\n",
    "                              'earlyspring2021_school_closures', 'spring2021_school_closures']].isna().sum()\n",
    "\n",
    "na_percentage = schools_closures[['LEAID', 'earlyfall2020_school_closures', 'fall2020_school_closures',\n",
    "                                  'earlyspring2021_school_closures', 'spring2021_school_closures']].isna().mean() * 100\n",
    "\n",
    "\n",
    "print(na_counts)\n",
    "print(na_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a32f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_FEATURES['LEAID'] = ZIP_FEATURES['LEAID'].astype(str)\n",
    "\n",
    "ZIP_FEATURES['LEAID'] = ZIP_FEATURES['LEAID'].apply(lambda x: '00' + x if len(x) == 5 else x)\n",
    "ZIP_FEATURES['LEAID'] = ZIP_FEATURES['LEAID'].apply(lambda x: '0' + x if len(x) == 6 else x)\n",
    "\n",
    "schools_closures['LEAID'] = schools_closures['LEAID'].apply(lambda x: '00' + x if len(x) == 5 else x)\n",
    "schools_closures['LEAID'] = schools_closures['LEAID'].apply(lambda x: '0' + x if len(x) == 6 else x)\n",
    "\n",
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, schools_closures, how = 'left', on = \"LEAID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f14d43",
   "metadata": {},
   "source": [
    "In terms of ZIP codes, this translates to big percentages. Over 20% of zip codes do not have data on school closures for the spring 2021 semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts = ZIP_FEATURES[['LEAID', 'earlyfall2020_school_closures', 'fall2020_school_closures',\n",
    "                              'earlyspring2021_school_closures', 'spring2021_school_closures']].isna().sum()\n",
    "\n",
    "na_percentage = ZIP_FEATURES[['LEAID', 'earlyfall2020_school_closures', 'fall2020_school_closures',\n",
    "                                  'earlyspring2021_school_closures', 'spring2021_school_closures']].isna().mean() * 100\n",
    "\n",
    "\n",
    "print(na_counts)\n",
    "print(na_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ca30f",
   "metadata": {},
   "source": [
    "I tried to consolidate the 2021 semester category for better coverage, but it made no difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d6b2ee",
   "metadata": {},
   "source": [
    "Maybe it doesn't make sense to have ZIP to CD and ZIP to County in the same dataset. I might make these into two later on, but for now this contains the county under which the ZIP code mostly falls for residential addresses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04631d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {\n",
    "    'FIPS_Code': str,\n",
    "    'District': str,\n",
    "    'CD':str\n",
    "}\n",
    "\n",
    "CD_df = pd.read_csv('data/House_Reps_District.csv', dtype=col_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a844a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_FEATURES['CD'] = ZIP_FEATURES['CD'].astype(str)\n",
    "CD_df['CD'] = CD_df['CD'].astype(str)\n",
    "\n",
    "\n",
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, CD_df, on = \"CD\", how= \"left\")\n",
    "\n",
    "ZIP_FEATURES = ZIP_FEATURES.rename(columns={\n",
    "    'COUNTY': 'COUNTY_FIPS', #coming from the HUD ZIP to County DF\n",
    "    'Name': 'CONGRESSPERSON',\n",
    "    'State': 'STATE',\n",
    "    'Party': 'CONGRESSPERSON_PARTY'\n",
    "})\n",
    "\n",
    "cols_to_move = ZIP_FEATURES.columns[3:5]\n",
    "df_remaining = ZIP_FEATURES.drop(columns=cols_to_move)\n",
    "ZIP_FEATURES = pd.concat([df_remaining, ZIP_FEATURES[cols_to_move]], axis=1)\n",
    "\n",
    "#ZIP_FEATURES['STATE'] = ZIP_FEATURES.apply(\n",
    "    #lambda row: row['PHYSICAL STATE'] if pd.isna(row['STATE']) and not pd.isna(row['PHYSICAL STATE']) else row['STATE'], \n",
    "    #axis=1\n",
    "#)\n",
    "\n",
    "#ZIP_FEATURES = ZIP_FEATURES.drop(columns='PHYSICAL STATE')\n",
    "\n",
    "ZIP_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {\n",
    "    'county_fips': str\n",
    "}\n",
    "\n",
    "mask_mandates_count = pd.read_excel('data/county_mask_mandate_data.xlsx', dtype=col_types)\n",
    "mask_mandates_count = mask_mandates_count[['county_fips','county_conditions','state_conditions']]\n",
    "\n",
    "mask_mandates_count['county_fips'] = mask_mandates_count['county_fips'].astype(str).str.zfill(5)\n",
    "\n",
    "mask_mandates_count['county_conditions'] = mask_mandates_count.apply(\n",
    "    lambda row: row['state_conditions'] if pd.isna(row['county_conditions']) and not pd.isna(row['state_conditions']) else row['county_conditions'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "mask_mandates_count['county_conditions'] = mask_mandates_count.apply(\n",
    "    lambda row: \"no mandate\" if pd.isna(row['county_conditions']) else row['county_conditions'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "mask_mandates_count = mask_mandates_count.rename(columns = {\n",
    "    'county_fips': 'COUNTY_FIPS'\n",
    "})\n",
    "\n",
    "print(mask_mandates_count.head())\n",
    "\n",
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, mask_mandates_count, on = \"COUNTY_FIPS\", how= \"left\")\n",
    "\n",
    "print(ZIP_FEATURES.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc99c4a",
   "metadata": {},
   "source": [
    "I have a pretty decent match rate of 95%. However, let's take a look at some of the non-matches and see if we can fill in the gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ZIP_FEATURES['CONGRESSPERSON'].isna().sum())\n",
    "print(ZIP_FEATURES['CONGRESSPERSON'].isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ZIP_FEATURES['county_conditions'].isna().sum())\n",
    "print(ZIP_FEATURES['county_conditions'].isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d304d05",
   "metadata": {},
   "source": [
    "Below, we need to add a leading \"0\" to the FIPS code (FIPS codes are all 5 digits) \\\n",
    "These are cumulative mortality numbers (not weekly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf80201",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {\n",
    "    'countyFIPS': str\n",
    "}\n",
    "\n",
    "usafacts_deaths = pd.read_csv('Data/Partisanship and Health Behavior/Data/covid_deaths_usafacts.csv', dtype=col_types)\n",
    "usafacts_deaths['countyFIPS'] = usafacts_deaths['countyFIPS'].astype(str).str.zfill(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2953de",
   "metadata": {},
   "source": [
    "For this preliminary analysis, I will only use wave 2,4,6. \\\n",
    "Each wave's data was collected on 2020-06-17 - 2020-06-23, 2020-11-29 - 2020-12-16, 2021-05-12 - 2021-05-25. \\\n",
    "I will take covid mortality from the midpoint of the wave range. \\\n",
    "Wave 2 = 6/18-24. 2020 \\\n",
    "Wave 3 = 9/12-20, 2020 \\\n",
    "Wave 4 = 12/1-7, 2020 \\\n",
    "Wave 5 = 2/9-14, 2021 \\\n",
    "Wave 6 = 5/14-25, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['countyFIPS','County Name',\n",
    "                   '2020-06-01','2020-06-15','2020-06-13','2020-06-20', #w2\n",
    "                   '2020-09-01','2020-09-16','2020-09-09','2020-09-05', #w3\n",
    "                   '2020-12-05','2020-12-04','2020-12-01','2020-11-28', #w4\n",
    "                   '2021-02-12','2021-02-10','2021-02-05','2021-01-29', #w5\n",
    "                   '2021-05-19','2021-05-17','2021-05-12','2021-05-05'] #w6\n",
    "\n",
    "usafacts_deaths = usafacts_deaths[columns_to_keep]\n",
    "\n",
    "new_column_names = [col if idx < 2 else 'COUNTY_COV_DEATHS_' + col.replace('-', '_') for idx, col in enumerate(columns_to_keep)]\n",
    "\n",
    "usafacts_deaths.columns = new_column_names\n",
    "\n",
    "usafacts_deaths = usafacts_deaths.rename(columns={\n",
    "    'countyFIPS': 'COUNTY_FIPS'\n",
    "})\n",
    "\n",
    "usafacts_deaths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09feb9",
   "metadata": {},
   "source": [
    "Nearly all ZIP codes get a match (less than 1 percent don't get matched). \\\n",
    "If we end up going back to mortality rates (as of now (01/14/24) we're using prevalence rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, usafacts_deaths, on = \"COUNTY_FIPS\", how = \"left\")\n",
    "\n",
    "print(ZIP_FEATURES['COUNTY_COV_DEATHS_2020_06_01'].isna().sum())\n",
    "print(ZIP_FEATURES['COUNTY_COV_DEATHS_2020_06_01'].isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05974d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types1 = {\n",
    "    'STATE': str,\n",
    "    'COUNTY': str\n",
    "}\n",
    "\n",
    "col_types2 = {\n",
    "    'County Code (FIPS)': str,\n",
    "    'County Subdivision Code (FIPS)': str,\n",
    "    'Place Code (FIPS)': str,\n",
    "    'Consolidtated City Code (FIPS)': str,\n",
    "    'State Code (FIPS)': str\n",
    "}\n",
    "\n",
    "\n",
    "census_pop = pd.read_csv('Data/Partisanship and Health Behavior/Data/co-est2021-alldata.csv', encoding='ISO-8859-1',\n",
    "                        dtype = col_types1)\n",
    "census_pop_FIPS = pd.read_excel('Data/Partisanship and Health Behavior/Data/all-geocodes-v2020.xlsx', \n",
    "                                dtype = col_types2)\n",
    "\n",
    "census_pop_FIPS = census_pop_FIPS.rename(columns={\n",
    "    'Area Name (including legal/statistical area description)': 'CTYNAME'\n",
    "})\n",
    "\n",
    "census_pop['COUNTY_FIPS'] = census_pop['STATE']+census_pop['COUNTY']\n",
    "\n",
    "State_FIPS_Crosswalk = census_pop[['STATE','STNAME']]\n",
    "State_FIPS_Crosswalk['STATE_FIPS'] = State_FIPS_Crosswalk['STATE'].astype(str)\n",
    "State_FIPS_Crosswalk['STATE'] = State_FIPS_Crosswalk['STNAME']\n",
    "\n",
    "State_FIPS_Crosswalk = State_FIPS_Crosswalk.drop(columns = ['STNAME'])\n",
    "\n",
    "State_FIPS_Crosswalk = State_FIPS_Crosswalk.drop_duplicates(subset='STATE', keep='first')\n",
    "State_FIPS_Crosswalk = State_FIPS_Crosswalk[pd.notna(State_FIPS_Crosswalk['STATE'])].reset_index(drop=True)\n",
    "\n",
    "State_FIPS_Crosswalk.to_csv('data/STATE_FIPS_CROSSWALK.csv', index = False)\n",
    "State_FIPS_Crosswalk.to_csv('data/Partisanship and Health Behavior/Data/STATE_FIPS_CROSSWALK.csv', index = False)\n",
    "\n",
    "columns_to_keep = ['STATE','POPESTIMATE2020','DEATHS2020','POPESTIMATE2021','DEATHS2021','COUNTY_FIPS']\n",
    "\n",
    "census_pop = census_pop[columns_to_keep]\n",
    "\n",
    "census_pop = census_pop.rename(columns={\n",
    "    'STATE': 'STATE_FIPS'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ceb2d",
   "metadata": {},
   "source": [
    "A crude mortality rate, not standardized or age adjusted, for this preliminary analysis \\\n",
    "I manually insert Puerto Rico' State FIPS code and District of Columbia to make this a clean merge across the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55dc616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, census_pop, on = \"COUNTY_FIPS\", how = \"left\")\n",
    "\n",
    "\n",
    "ZIP_FEATURES['COV_County_CMR_2020_06_20'] = (ZIP_FEATURES['COUNTY_COV_DEATHS_2020_06_20'] / ZIP_FEATURES['POPESTIMATE2020']) * 1000\n",
    "ZIP_FEATURES['COV_County_CMR_2020_12_09'] = (ZIP_FEATURES['COUNTY_COV_DEATHS_2020_12_05'] / ZIP_FEATURES['POPESTIMATE2020']) * 1000\n",
    "ZIP_FEATURES['COV_County_CMR_2021_05_19'] = (ZIP_FEATURES['COUNTY_COV_DEATHS_2021_05_19'] / ZIP_FEATURES['POPESTIMATE2021']) * 1000\n",
    "ZIP_FEATURES['STATE_FIPS'] = ZIP_FEATURES.apply(lambda row: 72 if row['STATE'] == \"Puerto Rico\" else row['STATE_FIPS'], axis=1)\n",
    "\n",
    "print(ZIP_FEATURES['COV_County_CMR_2020_06_20'].isna().sum())\n",
    "print(ZIP_FEATURES['COV_County_CMR_2020_06_20'].isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e71cd3",
   "metadata": {},
   "source": [
    "We have decided that prevalence is a better measure for predicting contact behavior than mortality \\\n",
    "I want these lags \\\n",
    "Wave 2 (6/18-24, 2020), midpoint: 06/21/2020, lag1: 06/14/2020, lag3: 06/07/2020 \\\n",
    "Wave 3 (9/12-20, 2020), midpoint: 09/16/2020, lag1: 09/09/2020, lag3: 09/02/2020 \\\n",
    "Wave 4 (12/1-7, 2020), midpoint: 12/04/2020, lag1: 11/27/2020, lag3: 11/20/2020 \\\n",
    "Wave 5 (2/9-14, 2021), midpoint: 02/11/2021, lag: 02/04/2021, lag3: 01/28/2021 \\\n",
    "Wave 6 (5/14-25, 2021), midpoint: 05/19/2021, lag: 05/12/2021, lag3: 05/05/2021 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a89499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b96d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_types = {\n",
    "    'FIPS': str,\n",
    "    'iso2':str\n",
    "}\n",
    "\n",
    "\n",
    "prevalence_mid_2020 = pd.read_csv('Data/Partisanship and Health Behavior/Data/JH_Prevalence/time_series_covid19_confirmed_US_mid_2020.csv', \n",
    "                                  dtype = col_types)\n",
    "\n",
    "columns_to_keep = ['FIPS','6/21/20','6/18/20','6/14/20','6/7/20']\n",
    "\n",
    "prevalence_mid_2020 = prevalence_mid_2020[prevalence_mid_2020['iso2'] == 'US']\n",
    "prevalence_mid_2020['FIPS'] = prevalence_mid_2020['FIPS'].apply(lambda x: str(x).rstrip('0').rstrip('.') if pd.notna(x) and '.' in str(x) else x)\n",
    "prevalence_mid_2020['FIPS'] = prevalence_mid_2020['FIPS'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "prevalence_mid_2020 = prevalence_mid_2020[columns_to_keep]\n",
    "\n",
    "#06/21/2020 - 06/14/2020\n",
    "prevalence_mid_2020['wave_2_incidence'] = prevalence_mid_2020['6/21/20'] - prevalence_mid_2020['6/14/20']\n",
    "\n",
    "prevalence_mid_2020.to_csv('data/wave_2_incidence.csv', index=False)\n",
    "\n",
    "prevalence_late_2020 =pd.read_csv('Data/Partisanship and Health Behavior/Data/JH_Prevalence/time_series_covid19_confirmed_US_late_2020.csv', \n",
    "                                  dtype = col_types)\n",
    "\n",
    "columns_to_keep = ['FIPS','9/16/20','9/12/20','9/9/20','9/2/20']\n",
    "\n",
    "prevalence_late_2020 = prevalence_late_2020[prevalence_late_2020['iso2'] == 'US']\n",
    "prevalence_late_2020['FIPS'] = prevalence_late_2020['FIPS'].apply(lambda x: str(x).rstrip('0').rstrip('.') if pd.notna(x) and '.' in str(x) else x)\n",
    "prevalence_late_2020['FIPS'] = prevalence_late_2020['FIPS'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "prevalence_late_2020 = prevalence_late_2020[columns_to_keep]\n",
    "\n",
    "#09/16/2020 - 09/09/2020\n",
    "prevalence_late_2020['wave_3_incidence'] = prevalence_late_2020['9/16/20'] - prevalence_late_2020['9/9/20']\n",
    "\n",
    "prevalence_late_2020.to_csv('data/wave_3_incidence.csv', index=False)\n",
    "\n",
    "prevalence_early_2021 = pd.read_csv('Data/Partisanship and Health Behavior/Data/JH_Prevalence/time_series_covid19_confirmed_US_early_2021.csv', \n",
    "                                  dtype = col_types)\n",
    "\n",
    "columns_to_keep = ['FIPS','12/4/20','12/1/20','11/27/20','11/20/20']\n",
    "\n",
    "prevalence_early_2021 = prevalence_early_2021[prevalence_early_2021['iso2'] == 'US']\n",
    "prevalence_early_2021['FIPS'] = prevalence_early_2021['FIPS'].apply(lambda x: str(x).rstrip('0').rstrip('.') if pd.notna(x) and '.' in str(x) else x)\n",
    "prevalence_early_2021['FIPS'] = prevalence_early_2021['FIPS'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "prevalence_early_2021 = prevalence_early_2021[columns_to_keep]\n",
    "\n",
    "prevalence_early_2021['wave_4_incidence'] = prevalence_early_2021['12/4/20'] - prevalence_early_2021['11/27/20']\n",
    "\n",
    "prevalence_early_2021.to_csv('data/wave_4_incidence.csv', index=False)\n",
    "\n",
    "prevalence_mid_2021 = pd.read_csv('Data/Partisanship and Health Behavior/Data/JH_Prevalence/time_series_covid19_confirmed_US_mid_2021.csv', \n",
    "                                  dtype = col_types)\n",
    "\n",
    "columns_to_keep = ['FIPS','2/11/21','2/9/21','2/4/21','1/28/21']\n",
    "\n",
    "prevalence_mid_2021 = prevalence_mid_2021[prevalence_mid_2021['iso2'] == 'US']\n",
    "prevalence_mid_2021['FIPS'] = prevalence_mid_2021['FIPS'].apply(lambda x: str(x).rstrip('0').rstrip('.') if pd.notna(x) and '.' in str(x) else x)\n",
    "prevalence_mid_2021['FIPS'] = prevalence_mid_2021['FIPS'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "prevalence_mid_2021 = prevalence_mid_2021[columns_to_keep]\n",
    "\n",
    "#02/11/2021 - 02/04/2021\n",
    "prevalence_mid_2021['wave_5_incidence'] = prevalence_mid_2021['2/11/21'] - prevalence_mid_2021['2/4/21']\n",
    "\n",
    "prevalence_mid_2021.to_csv('data/wave_5_incidence.csv', index=False)\n",
    "\n",
    "prevalence_mid_2021_2 = pd.read_csv('Data/Partisanship and Health Behavior/Data/JH_Prevalence/time_series_covid19_confirmed_US_mid_2021.csv', \n",
    "                                  dtype = col_types)\n",
    "\n",
    "colums_to_keep = ['FIPS','5/19/21','5/14/21','5/12/21','5/5/21']\n",
    "\n",
    "prevalence_mid_2021_2 = prevalence_mid_2021_2[colums_to_keep]\n",
    "\n",
    "#05/19/2021 - 05/12/2021\n",
    "prevalence_mid_2021_2['wave_6_incidence'] = prevalence_mid_2021_2['5/19/21'] - prevalence_mid_2021_2['5/12/21']\n",
    "\n",
    "prevalence_mid_2021_2.to_csv('data/wave_6_incidence.csv', index=False)\n",
    "\n",
    "prevalence = pd.merge(prevalence_mid_2020, prevalence_late_2020, how = \"left\", on = \"FIPS\")\n",
    "prevalence = pd.merge(prevalence, prevalence_early_2021, how = \"left\", on = \"FIPS\")\n",
    "prevalence = pd.merge(prevalence, prevalence_mid_2021, how = \"left\", on = \"FIPS\")\n",
    "prevalence = pd.merge(prevalence, prevalence_mid_2021_2, how = 'left', on = \"FIPS\")\n",
    "prevalence.columns = [col if i == 0 else 'prev_' + col for i, col in enumerate(prevalence.columns)]\n",
    "\n",
    "prevalence = prevalence.rename(columns={\n",
    "    'FIPS':'COUNTY_FIPS'\n",
    "})\n",
    "\n",
    "prevalence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5194067a",
   "metadata": {},
   "source": [
    "Next, I want to create the weekly incidence for each key date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac08d7a",
   "metadata": {},
   "source": [
    "#06/21/2020 - 06/14/2020\n",
    "prevalence['wave_2_incidence'] = prevalence['prev_6/21/20'] - prevalence['prev_6/14/20']\n",
    "#09/16/2020 - 09/09/2020\n",
    "prevalence['wave_3_incidence'] = prevalence['prev_9/16/20'] - prevalence['prev_9/9/20']\n",
    "#12/04/2020 - 11/27/2020\n",
    "prevalence['wave_4_incidence'] = prevalence['prev_12/4/20'] - prevalence['prev_11/27/20']\n",
    "#02/11/2021 - 02/04/2021\n",
    "prevalence['wave_5_incidence'] = prevalence['prev_2/11/21'] - prevalence['prev_2/4/21']\n",
    "#05/19/2021 - 05/12/2021\n",
    "prevalence['wave_6_incidence'] = prevalence['prev_5/19/21'] - prevalence['prev_5/12/21']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c05fa",
   "metadata": {},
   "source": [
    "below we turn these figures into rates based on population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0094a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, prevalence, on = \"COUNTY_FIPS\", how = \"left\")\n",
    "\n",
    "for col in ZIP_FEATURES.columns:\n",
    "    # Check if the column name starts with 'prev_'\n",
    "    if col.startswith('prev_'):\n",
    "        # Extract the date part of the column name\n",
    "        date_part = col.split('_', 1)[1]\n",
    "        # Perform the calculation and create a new column\n",
    "        ZIP_FEATURES['rate_prev_' + date_part] = (ZIP_FEATURES[col] / ZIP_FEATURES['POPESTIMATE2020']) * 1000\n",
    "        \n",
    "prev_columns = prevalence.columns\n",
    "prev_columns = prev_columns[1:]\n",
    "ZIP_FEATURES = ZIP_FEATURES.drop(columns=prev_columns)\n",
    "\n",
    "print(ZIP_FEATURES['rate_prev_11/27/20'].isna().sum())\n",
    "print(ZIP_FEATURES['rate_prev_11/27/20'].isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c2496",
   "metadata": {},
   "source": [
    "Below I insert data on urbanicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19fc43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['FIPS','RUCC_2013']\n",
    "\n",
    "col_types = {\n",
    "    'FIPS': str,\n",
    "}\n",
    "\n",
    "RUC_county_df = pd.read_csv('Data/Partisanship and Health Behavior/Data/ruralurbancodes2013.csv', dtype = col_types)\n",
    "\n",
    "RUC_county_df = RUC_county_df[columns_to_keep]\n",
    "\n",
    "RUC_county_df = RUC_county_df.rename(columns={\n",
    "    'RUCC_2013': 'COUNTY_RUCC_2013',\n",
    "    'FIPS':'COUNTY_FIPS'\n",
    "})\n",
    "\n",
    "RUC_county_df['COUNTY_FIPS'] = RUC_county_df['COUNTY_FIPS'].astype(str)\n",
    "\n",
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, RUC_county_df, on = \"COUNTY_FIPS\", how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd442686",
   "metadata": {},
   "source": [
    "Next, I want to control for the severity of the lockdowns according to Oxford's tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e17cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "State_Restrictions_df = pd.read_csv('Data/Partisanship and Health Behavior/Data/OxCGRT_USA_differentiated_withnotes_2020.csv')\n",
    "State_Restrictions_df['Date'] = State_Restrictions_df['Date'].astype(str)\n",
    "State_Restrictions_df = State_Restrictions_df[State_Restrictions_df['Date'].isin(['20200617', '20201129'])]\n",
    "columns_to_keep = ['RegionName','GovernmentResponseIndex_SimpleAverage','Date']\n",
    "State_Restrictions_df = State_Restrictions_df[columns_to_keep]\n",
    "\n",
    "State_Restrictions_df = State_Restrictions_df.rename(columns={\n",
    "    'RegionName': 'STATE',\n",
    "    'GovernmentResponseIndex_SimpleAverage':'State_Government_Response_Index'\n",
    "})\n",
    "\n",
    "State_Restrictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b795cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "State_Restrictions_df['State_Government_Response_Index_06172020'] = State_Restrictions_df.apply(\n",
    "    lambda row: row['State_Government_Response_Index'] if row['Date'] == \"20200617\" else \"0\", axis=1\n",
    ")\n",
    "\n",
    "State_Restrictions_df['State_Government_Response_Index_06172020'] = State_Restrictions_df.apply(\n",
    "    lambda row: row['State_Government_Response_Index'] if row['State_Government_Response_Index_06172020'] == \"0\" else row['State_Government_Response_Index_06172020'], axis=1\n",
    ")\n",
    "\n",
    "State_Restrictions_df['State_Government_Response_Index_11192020'] = State_Restrictions_df.apply(\n",
    "    lambda row: row['State_Government_Response_Index'] if row['Date'] == \"20201129\" else \"0\", axis=1\n",
    ")\n",
    "\n",
    "State_Restrictions_df['State_Government_Response_Index_11192020'] = State_Restrictions_df.apply(\n",
    "    lambda row: row['State_Government_Response_Index'] if row['State_Government_Response_Index_11192020'] == \"0\" else row['State_Government_Response_Index_11192020'], axis=1\n",
    ")\n",
    "\n",
    "State_Restrictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a5c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, State_Restrictions_df, on = \"STATE\", how = \"left\")\n",
    "\n",
    "ZIP_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f816fbd",
   "metadata": {},
   "source": [
    "Next, I want to read in the county level presidential data and generate two variables \\\n",
    "1. For which president did the county overall vote for?\n",
    "2. What percentage of the county voted for which president?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188950dd",
   "metadata": {},
   "source": [
    "note: I manually added in DC because it was not in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66e46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {\n",
    "    'county_fips': str,\n",
    "}\n",
    "county_pres = pd.read_csv('Data/Partisanship and Health Behavior/Data/countypres_2000-2020.csv', dtype = col_types)\n",
    "county_pres = county_pres[county_pres['year'] == 2020].reset_index(drop=True)\n",
    "\n",
    "county_pres['party_fips'] = county_pres['party'].astype(str) + county_pres['county_fips'].astype(str)\n",
    "\n",
    "#there was an issue where there were multiple types of votes on each line for the same person\n",
    "total_votes_by_party = county_pres.groupby('party_fips')['candidatevotes'].sum().reset_index()\n",
    "total_votes_by_party.rename(columns={'candidatevotes': 'total_candidatevotes'}, inplace=True)\n",
    "\n",
    "county_pres = county_pres.merge(total_votes_by_party, on='party_fips', how='left')\n",
    "\n",
    "county_pres['percentage_won'] = (county_pres['total_candidatevotes'] / county_pres['totalvotes'])*100\n",
    "\n",
    "# Group by 'countyfips' and find the index of the row with the largest 'percentage_won' in each group\n",
    "county_pres_wide = county_pres.groupby('county_fips')['percentage_won'].idxmax()\n",
    "\n",
    "# Select rows from the original DataFrame corresponding to these indices\n",
    "county_pres_wide = county_pres.loc[county_pres_wide]\n",
    "\n",
    "county_pres_wide = county_pres_wide.rename(columns={\n",
    "    'candidate': 'winning_pres_candidate',\n",
    "    'party':'winning_pres_party',\n",
    "    'percentage_won': 'winning_pres_candidate_percentage'\n",
    "})\n",
    "\n",
    "#merge in the dems\n",
    "dem_county_pres = county_pres[county_pres['party'] == 'DEMOCRAT']\n",
    "\n",
    "dem_county_pres.rename(columns={'percentage_won': 'biden_percentage_won'}, inplace=True)\n",
    "\n",
    "dem_county_pres = dem_county_pres[['biden_percentage_won','county_fips']]\n",
    "\n",
    "county_pres_wide = county_pres_wide.merge(dem_county_pres, on='county_fips', how='left')\n",
    "\n",
    "#merge in the reps\n",
    "rep_county_pres = county_pres[county_pres['party'] == 'REPUBLICAN']\n",
    "\n",
    "rep_county_pres.rename(columns={'percentage_won': 'trump_percentage_won'}, inplace=True)\n",
    "\n",
    "rep_county_pres = rep_county_pres[['trump_percentage_won','county_fips']]\n",
    "\n",
    "county_pres_wide = county_pres_wide.merge(rep_county_pres, on='county_fips', how='left')\n",
    "\n",
    "#keeping only the columns I want\n",
    "county_pres_wide = county_pres_wide.drop(columns =['year','state','state_po','county_name','office','candidatevotes','totalvotes','version','mode','party_fips'])\n",
    "\n",
    "county_pres_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_pres_wide = county_pres_wide.rename(columns={\n",
    "    'county_fips': 'COUNTY_FIPS'\n",
    "})\n",
    "\n",
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, county_pres_wide, on = \"COUNTY_FIPS\", how = \"left\")\n",
    "\n",
    "ZIP_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b538f",
   "metadata": {},
   "source": [
    "Next, for the sake of consistency, I want to get data on the percentage of the CD that was won by a democrat and Republican, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd01f3f",
   "metadata": {},
   "source": [
    "However, I need a clean crosswalk (as this one is pulling out several duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83926e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {\n",
    "    'DISTRICT': str,\n",
    "}\n",
    "\n",
    "county_CD_share = pd.read_excel('Data/Partisanship and Health Behavior/Data/federalelections2020.xlsx', \n",
    "                                sheet_name='13_US_House_Results_State', \n",
    "                                dtype=col_types)\n",
    "\n",
    "county_CD_share = county_CD_share.rename(columns={\n",
    "    'DISTRICT': 'CD_DISTRICT'\n",
    "})\n",
    "\n",
    "county_CD_share = county_CD_share[pd.notna(county_CD_share['GENERAL %'])].reset_index(drop=True)\n",
    "\n",
    "county_CD_share = pd.merge(county_CD_share, State_FIPS_Crosswalk, how='left', on=\"STATE\")\n",
    "\n",
    "county_CD_share['STATE_FIPS'] = county_CD_share['STATE_FIPS'].astype(str)\n",
    "county_CD_share['CD_DISTRICT'] = county_CD_share['CD_DISTRICT'].astype(str)\n",
    "\n",
    "county_CD_share['STATE_FIPS'] = county_CD_share['STATE_FIPS'].apply(lambda x: '0' + x if len(x) == 1 else x)\n",
    "county_CD_share['CD_DISTRICT'] = county_CD_share['CD_DISTRICT'].apply(lambda x: '0' + x if len(x) == 1 else x)\n",
    "\n",
    "county_CD_share['CD'] = county_CD_share['STATE_FIPS'] + county_CD_share['CD_DISTRICT']\n",
    "\n",
    "county_CD_share.to_csv('data/CD_COUNTY_PERCENT_WON.csv')\n",
    "\n",
    "D_county_CD_share = county_CD_share[county_CD_share['PARTY'].str.contains(\"D\")].reset_index(drop=True)\n",
    "D_county_CD_share = D_county_CD_share[['GENERAL %','CD']]\n",
    "\n",
    "D_county_CD_share = D_county_CD_share.rename(columns={\n",
    "    'GENERAL %': 'CD_PERCENT_DEMOCRAT'\n",
    "})\n",
    "\n",
    "R_county_CD_share = county_CD_share[county_CD_share['PARTY'].str.contains(\"R\")].reset_index(drop=True)\n",
    "R_county_CD_share = R_county_CD_share[['GENERAL %','CD']]\n",
    "\n",
    "R_county_CD_share = R_county_CD_share.rename(columns={\n",
    "    'GENERAL %': 'CD_PERCENT_REPUBLICAN'\n",
    "})\n",
    "\n",
    "D_county_CD_share = D_county_CD_share[D_county_CD_share['CD'].notna()]\n",
    "R_county_CD_share = R_county_CD_share[R_county_CD_share['CD'].notna()]\n",
    "\n",
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, D_county_CD_share, how = 'left', on = \"CD\")\n",
    "ZIP_FEATURES = pd.merge(ZIP_FEATURES, R_county_CD_share, how = 'left', on = \"CD\")\n",
    "\n",
    "print(ZIP_FEATURES['CD_PERCENT_REPUBLICAN'].isna().sum())\n",
    "print(ZIP_FEATURES['CD_PERCENT_REPUBLICAN'].isna().mean() * 100)\n",
    "\n",
    "#manually adjusting these rows that received 0 votes in democratic counties\n",
    "ZIP_FEATURES['CD_PERCENT_REPUBLICAN'] = ZIP_FEATURES.apply(\n",
    "    lambda row: 0 if pd.notna(row['CD_PERCENT_DEMOCRAT']) and pd.isna(row['CD_PERCENT_REPUBLICAN']) else row['CD_PERCENT_REPUBLICAN'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#now in republican counties\n",
    "ZIP_FEATURES['CD_PERCENT_DEMOCRAT'] = ZIP_FEATURES.apply(\n",
    "    lambda row: 0 if row['CD_PERCENT_REPUBLICAN'] != 0 and pd.notna(row['CD_PERCENT_REPUBLICAN']) and pd.isna(row['CD_PERCENT_DEMOCRAT']) else row['CD_PERCENT_DEMOCRAT'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "ZIP_FEATURES['CD'] = ZIP_FEATURES.apply(lambda row: 0 if row['CD'] == \"1225\" else row['CD'],\n",
    "                                                        axis=1)\n",
    "\n",
    "ZIP_FEATURES['CD'] = ZIP_FEATURES.apply(lambda row: 100 if row['CD'] == \"1225\" else row['CD'],\n",
    "                                                        axis=1)\n",
    "\n",
    "print(ZIP_FEATURES['CD_PERCENT_REPUBLICAN'].isna().sum())\n",
    "print(ZIP_FEATURES['CD_PERCENT_REPUBLICAN'].isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152462c9",
   "metadata": {},
   "source": [
    "Lastly, I want to merge in demographic data by CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {\n",
    "    'GEO_ID': str,\n",
    "}\n",
    "\n",
    "CD_demographics = pd.read_csv('data/ACSDP5Y2020.DP05-Data.csv', dtype=col_types, header=1)\n",
    "\n",
    "CD_demographics = CD_demographics[['Geography', 'Geographic Area Name', 'Estimate!!SEX AND AGE!!Total population', \n",
    "                                   'Estimate!!SEX AND AGE!!Total population!!Female', 'Estimate!!SEX AND AGE!!Total population!!Sex ratio (males per 100 females)',\n",
    "                                  'Estimate!!SEX AND AGE!!Total population!!Under 5 years', 'Estimate!!SEX AND AGE!!Total population!!65 to 74 years',\n",
    "                                  'Estimate!!SEX AND AGE!!Total population!!75 to 84 years','Estimate!!SEX AND AGE!!Total population!!85 years and over',\n",
    "                                  'Estimate!!SEX AND AGE!!Total population!!Median age (years)','Estimate!!RACE!!Total population',\n",
    "                                  'Estimate!!RACE!!Total population!!One race','Estimate!!RACE!!Total population!!Two or more races',\n",
    "                                  'Estimate!!RACE!!Total population!!One race!!White','Estimate!!RACE!!Total population!!One race!!Black or African American',\n",
    "                                  'Estimate!!RACE!!Total population!!One race!!American Indian and Alaska Native','Estimate!!RACE!!Total population!!One race!!Asian',\n",
    "                                  'Estimate!!RACE!!Total population!!One race!!Asian!!Asian Indian','Margin of Error!!Race alone or in combination with one or more other races!!Total population!!White',\n",
    "                                  'Estimate!!Race alone or in combination with one or more other races!!Total population!!Black or African American',\n",
    "                                  'Estimate!!Race alone or in combination with one or more other races!!Total population!!American Indian and Alaska Native',\n",
    "                                  'Estimate!!Race alone or in combination with one or more other races!!Total population!!Asian',\n",
    "                                  'Estimate!!HISPANIC OR LATINO AND RACE!!Total population',\n",
    "                                  'Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)',\n",
    "                                  'Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Not Hispanic or Latino',\n",
    "                                  'Percent!!SEX AND AGE!!Total population!!65 years and over',\n",
    "                                  'Percent!!RACE!!Total population!!One race','Percent!!RACE!!Total population!!Two or more races',\n",
    "                                  'Percent!!RACE!!Total population!!One race!!White','Percent!!RACE!!Total population!!One race!!Black or African American',\n",
    "                                  'Percent!!RACE!!Total population!!One race!!American Indian and Alaska Native','Percent!!RACE!!Total population!!One race!!Asian',\n",
    "                                  'Percent!!RACE!!Total population!!One race!!Asian!!Asian Indian','Percent!!Race alone or in combination with one or more other races!!Total population!!White',\n",
    "                                  'Percent!!Race alone or in combination with one or more other races!!Total population!!Black or African American',\n",
    "                                  'Percent!!Race alone or in combination with one or more other races!!Total population!!American Indian and Alaska Native',\n",
    "                                  'Percent!!Race alone or in combination with one or more other races!!Total population!!Asian',\n",
    "                                  'Percent!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)',\n",
    "                                  'Percent!!HISPANIC OR LATINO AND RACE!!Total population!!Not Hispanic or Latino']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e34bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_FEATURES.to_csv('data/ZIP_Features.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acec07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_FEATURES.to_csv('data/Partisanship and Health Behavior/Data/ZIP_Features.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc51fa3",
   "metadata": {},
   "source": [
    "Why is resp_educ and education different?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd1d31",
   "metadata": {},
   "source": [
    "Now I'll merge to the BICS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0bee07c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/6bdxzk2j30v5n3wstywbcpg80000gn/T/ipykernel_52145/3714840210.py:11: DtypeWarning: Columns (84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_w6 = pd.read_csv('/Users/chrissoria/Documents/Research/BICS/data/national_wave6_unweighted.csv', dtype = col_types)\n"
     ]
    }
   ],
   "source": [
    "col_types = {\n",
    "    'resp_zip': str,\n",
    "    'political_party': str\n",
    "}\n",
    "\n",
    "df_w1 = pd.read_csv('/Users/chrissoria/Documents/Research/BICS/data/national_wave1_unweighted.csv', dtype = col_types)\n",
    "df_w2 = pd.read_csv('/Users/chrissoria/Documents/Research/BICS/data/national_wave2_unweighted.csv', dtype = col_types)\n",
    "df_w3 = pd.read_csv('/Users/chrissoria/Documents/Research/BICS/data/national_wave3_unweighted.csv', dtype = col_types)\n",
    "df_w4 = pd.read_csv('/Users/chrissoria/Documents/Research/BICS/data/national_wave4_unweighted.csv', dtype = col_types)\n",
    "df_w5 = pd.read_csv('/Users/chrissoria/Documents/Research/BICS/data/national_wave5_unweighted.csv', dtype = col_types)\n",
    "df_w6 = pd.read_csv('/Users/chrissoria/Documents/Research/BICS/data/national_wave6_unweighted.csv', dtype = col_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7dbacf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"resp_employ\" in df_w2.columns:\n",
    "    df_w2[\"resp_employ\"] = df_w2[\"resp_employ\"].astype(str)\n",
    "\n",
    "if \"resp_employ\" in df_w3.columns:\n",
    "    df_w3[\"resp_employ\"] = df_w3[\"resp_employ\"].astype(str)    \n",
    "    \n",
    "if \"resp_employ\" in df_w4.columns:\n",
    "    df_w4[\"resp_employ\"] = df_w4[\"resp_employ\"].astype(str)\n",
    "    \n",
    "if \"resp_employ\" in df_w5.columns:\n",
    "    df_w5[\"resp_employ\"] = df_w5[\"resp_employ\"].astype(str)\n",
    "\n",
    "if \"resp_employ\" in df_w6.columns:\n",
    "    df_w6[\"resp_employ\"] = df_w6[\"resp_employ\"].astype(str)\n",
    "    \n",
    "df_w2['resp_occupation'] = df_w2['resp_occupation'].astype(str)\n",
    "df_w4['resp_occupation'] = df_w4['resp_occupation'].astype(str)\n",
    "\n",
    "df_combined = pd.concat([df_w2, df_w3, df_w4, df_w5, df_w6], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741abd3",
   "metadata": {},
   "source": [
    "Below I'm converting the waves into date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cebb7a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/6bdxzk2j30v5n3wstywbcpg80000gn/T/ipykernel_52145/1458976774.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_combined['data_collected_dates'] = df_combined.apply(lambda row: \"June 18-24, '20\" if row['wave'] == 2\n"
     ]
    }
   ],
   "source": [
    "df_combined['data_collected_dates'] = df_combined.apply(lambda row: \"June 18-24, '20\" if row['wave'] == 2\n",
    "                                                       else \"September 12-20, '20'\" if row['wave'] == 3\n",
    "                                                       else \"December 1-7, '20'\" if row['wave'] == 4\n",
    "                                                       else \"February 9-14, '21'\" if row['wave'] == 5\n",
    "                                                       else \"May 14-25, '21'\" if row['wave'] == 6\n",
    "                                                        else np.nan, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c1c39b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_collected_dates\n",
      "May 14-25, '21'          5418\n",
      "September 12-20, '20'    3248\n",
      "December 1-7, '20'       2993\n",
      "February 9-14, '21'      2976\n",
      "June 18-24, '20          2432\n",
      "Name: count, dtype: int64\n",
      "wave\n",
      "6    5418\n",
      "3    3248\n",
      "4    2993\n",
      "5    2976\n",
      "2    2432\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_combined['data_collected_dates'].value_counts())\n",
    "print(df_combined['wave'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181c171",
   "metadata": {},
   "source": [
    "We're missing a \"conservative\" but we have two moderates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4fe8aeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/6bdxzk2j30v5n3wstywbcpg80000gn/T/ipykernel_52145/2967877890.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_combined['political_view_numeric'] = df_combined['political_view'].replace(political_view_mapping)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "political_view\n",
       "Moderate                  4542\n",
       "Liberal                   2695\n",
       "Slightly conservative     2277\n",
       "Extremely liberal         2262\n",
       "Extremely conservative    2021\n",
       "Middle of the road        1779\n",
       "Slightly liberal          1491\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political_view_mapping = {\n",
    "    'Extremely conservative': 1,\n",
    "    'Slightly conservative': 2,\n",
    "    'Moderate': 3,\n",
    "    'Middle of the road': 3,\n",
    "    'Slightly liberal': 4,\n",
    "    'Liberal': 5,\n",
    "    'Extremely liberal': 6\n",
    "}\n",
    "\n",
    "# Creating a new variable 'political_view_numeric' by mapping the 'political_view' column using the defined mapping\n",
    "df_combined['political_view_numeric'] = df_combined['political_view'].replace(political_view_mapping)\n",
    "\n",
    "df_combined['political_view'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1a516ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/6bdxzk2j30v5n3wstywbcpg80000gn/T/ipykernel_52145/3210909615.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_combined['educ_group'] = df_combined.apply(categorize_education, axis=1)\n",
      "/var/folders/89/6bdxzk2j30v5n3wstywbcpg80000gn/T/ipykernel_52145/3210909615.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_combined['r_race'] = df_combined.apply(categorize_race, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_race\n",
      "White            13226\n",
      "Black             1882\n",
      "Asian             1180\n",
      "Other / Mixed      779\n",
      "Name: count, dtype: int64\n",
      "r_working\n",
      "Working        11372\n",
      "Not Working     5695\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/6bdxzk2j30v5n3wstywbcpg80000gn/T/ipykernel_52145/3210909615.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_combined['r_working'] = np.where(df_combined['industry'] == \"I don't work\", \"Not Working\", \"Working\")\n"
     ]
    }
   ],
   "source": [
    "#education variable\n",
    "def categorize_education(row):\n",
    "    if row['resp_educ'] == \"Less than high school degree\":\n",
    "        return \"Less than high school\"\n",
    "    elif row['resp_educ'] in [\"High school graduate (high school diploma or equivalent including GED)\", \"Some college but no degree\", \"Associate degree in college (2-year)\"]:\n",
    "        return \"High school graduate\"\n",
    "    elif row['resp_educ'] in [\"Bachelor's degree in college (4-year)\", \"Master's degree\", \"Doctoral degree\", \"Professional degree (JD, MD)\"]:\n",
    "        return \"College graduate and above\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "df_combined['educ_group'] = df_combined.apply(categorize_education, axis=1)\n",
    "\n",
    "df_combined['educ_group'].value_counts(dropna=False)\n",
    "\n",
    "race_columns = [col for col in df_combined.columns if col.startswith('resp_race_')]\n",
    "df_combined[race_columns] = df_combined[race_columns].fillna(0)\n",
    "\n",
    "#race variable\n",
    "def categorize_race(row):\n",
    "    if row['resp_race_1'] == \"White\":\n",
    "        return \"White\"\n",
    "    elif row['resp_race_2'] == \"Black or African American\":\n",
    "        return \"Black\"\n",
    "    elif row['resp_race_4'] == \"Asian\":\n",
    "        return \"Asian\"\n",
    "    else:\n",
    "        return \"Other / Mixed\"\n",
    "\n",
    "df_combined['r_race'] = df_combined.apply(categorize_race, axis=1)\n",
    "\n",
    "print(df_combined['r_race'].value_counts(dropna=False))\n",
    "\n",
    "#employment variable\n",
    "employment_columns = [col for col in df_combined.columns if col.startswith('resp_employ_')]\n",
    "df_combined[employment_columns] = df_combined[employment_columns].fillna(0)\n",
    "\n",
    "# Create the 'r_working' var based whether reported\n",
    "df_combined['r_working'] = np.where(df_combined['industry'] == \"I don't work\", \"Not Working\", \"Working\")\n",
    "\n",
    "print(df_combined['r_working'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf72612",
   "metadata": {},
   "source": [
    "dependent variable for concern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5ce3edea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/6bdxzk2j30v5n3wstywbcpg80000gn/T/ipykernel_52145/3902376750.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_combined['binary_concern'] = df_combined['covid19_concern'].apply(\n",
      "/var/folders/89/6bdxzk2j30v5n3wstywbcpg80000gn/T/ipykernel_52145/3902376750.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_combined['binary_concern_strong'] = df_combined['covid19_concern'].apply(\n",
      "/var/folders/89/6bdxzk2j30v5n3wstywbcpg80000gn/T/ipykernel_52145/3902376750.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_combined['contact_reduction'] = df_combined['covid19_f2fchange'].apply(\n"
     ]
    }
   ],
   "source": [
    "df_combined['binary_concern'] = df_combined['covid19_concern'].apply(\n",
    "    lambda x: 1 if x in [\"Somewhat concerned\", \"Very concerned\"] else (0 if pd.notna(x) else None)\n",
    ")\n",
    "\n",
    "df_combined['binary_concern_strong'] = df_combined['covid19_concern'].apply(\n",
    "    lambda x: 1 if x == \"Very concerned\" else (0 if pd.notna(x) else None)\n",
    ")\n",
    "\n",
    "df_combined['contact_reduction'] = df_combined['covid19_f2fchange'].apply(\n",
    "    lambda x: 1 if x == \"I have greatly reduced face-to-face interaction with others\" else (0 if pd.notna(x) else None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a5c4e",
   "metadata": {},
   "source": [
    "reading in the alter files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a9b497ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w2_nonhhalters = pd.read_csv(\"/Users/chrissoria/documents/research/BICs/data/national_alters_nonhh_wave2_unweighted.csv\")\n",
    "df_w3_nonhhalters = pd.read_csv(\"/Users/chrissoria/documents/research/BICs/data/national_alters_nonhh_wave3_unweighted.csv\")\n",
    "df_w4_nonhhalters = pd.read_csv(\"/Users/chrissoria/documents/research/BICs/data/national_alters_nonhh_wave4_unweighted.csv\")\n",
    "df_w5_nonhhalters = pd.read_csv(\"/Users/chrissoria/documents/research/BICs/data/national_alters_nonhh_wave5_unweighted.csv\")\n",
    "df_w6_nonhhalters = pd.read_csv(\"/Users/chrissoria/documents/research/BICs/data/national_alters_nonhh_wave6_unweighted.csv\")\n",
    "\n",
    "df_alters_combined = pd.concat([df_w2_nonhhalters, df_w3_nonhhalters, df_w4_nonhhalters, df_w5_nonhhalters, df_w6_nonhhalters], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "df0734c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary indicators for mask, gloves, and other protective equipment usage\n",
    "df_alters_combined['Mask_Used'] = (\n",
    "    ((df_alters_combined['protection_1'] == \"Wear a face mask\") |\n",
    "    (df_alters_combined['protection_2'] == \"Wear a face mask\") |\n",
    "    (df_alters_combined['protection_3'] == \"Wear a face mask\") |\n",
    "    (df_alters_combined['protection_4'] == \"Wear a face mask\")).astype(int)\n",
    ")\n",
    "\n",
    "df_alters_combined['Gloves_Used'] = (\n",
    "    ((df_alters_combined['protection_1'] == \"Wear gloves\") |\n",
    "    (df_alters_combined['protection_2'] == \"Wear gloves\") |\n",
    "    (df_alters_combined['protection_3'] == \"Wear gloves\") |\n",
    "    (df_alters_combined['protection_4'] == \"Wear gloves\")).astype(int)\n",
    ")\n",
    "\n",
    "df_alters_combined['Other_Protective_Equipment_Used'] = (\n",
    "    ((df_alters_combined['protection_1'] == \"Wear other protective equipment\") |\n",
    "    (df_alters_combined['protection_2'] == \"Wear other protective equipment\") |\n",
    "    (df_alters_combined['protection_3'] == \"Wear other protective equipment\") |\n",
    "    (df_alters_combined['protection_4'] == \"Wear other protective equipment\")).astype(int)\n",
    ")\n",
    "\n",
    "# Aggregate these indicators at the 'rid' level\n",
    "df_aggregated = df_alters_combined.groupby('rid').agg(\n",
    "    Total_Masks_Used=('Mask_Used', 'sum'),\n",
    "    Total_Gloves_Used=('Gloves_Used', 'sum'),\n",
    "    Total_Other_Protective_Equipment_Used=('Other_Protective_Equipment_Used', 'sum'),\n",
    "    Contacts=('rid', 'size')\n",
    ").reset_index()\n",
    "\n",
    "# Normalize the counts by the number of contacts\n",
    "df_aggregated['Norm_Masks_Used'] = df_aggregated['Total_Masks_Used'] / df_aggregated['Contacts']\n",
    "df_aggregated['Norm_Gloves_Used'] = df_aggregated['Total_Gloves_Used'] / df_aggregated['Contacts']\n",
    "df_aggregated['Norm_Other_Protective_Equipment_Used'] = df_aggregated['Total_Other_Protective_Equipment_Used'] / df_aggregated['Contacts']\n",
    "\n",
    "# Calculate non-weighted and weighted safety indices\n",
    "df_aggregated['Non_Weighted_Safety_Index'] = (\n",
    "    df_aggregated['Norm_Masks_Used'] + \n",
    "    df_aggregated['Norm_Gloves_Used'] + \n",
    "    df_aggregated['Norm_Other_Protective_Equipment_Used']\n",
    ") / 3\n",
    "\n",
    "df_aggregated['Weighted_Safety_Index'] = (\n",
    "    df_aggregated['Norm_Masks_Used'] + \n",
    "    df_aggregated['Norm_Gloves_Used'] + \n",
    "    df_aggregated['Norm_Other_Protective_Equipment_Used']\n",
    ") / (3 * df_aggregated['Contacts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4610d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_contacts = pd.merge(df_combined, df_aggregated, on = \"rid\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7e66c35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wave\n",
       "6    5418\n",
       "3    3248\n",
       "4    2993\n",
       "5    2976\n",
       "2    2432\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_contacts['wave'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "615239c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "political_party\n",
       "Democrat                7361\n",
       "Republican              4814\n",
       "Independent             3968\n",
       "Prefer not to answer     924\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_contacts['political_party'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3e0e80b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_contacts.to_csv('data/BICS_ego_alters_merged.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5ab44200",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m col_types \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZIP\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCD\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCONGRESSPERSON_PARTY\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m ZIP_FEATURES \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/chrissoria/Documents/Research/BICS_Political_Polarization/data/ZIP_Features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m                           dtype\u001b[38;5;241m=\u001b[39mcol_types)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m         nrows\n\u001b[1;32m   1706\u001b[0m     )\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:1049\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/common.py:1335\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;66;03m#  here too.\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m     \u001b[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1331\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[1;32m   1332\u001b[0m     )\n\u001b[0;32m-> 1335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "col_types = {\n",
    "    'ZIP': str,\n",
    "    'CD':str,\n",
    "    'COUNTY_FIPS':str,\n",
    "    'STATE_FIPS':str,\n",
    "    'CONGRESSPERSON_PARTY': str\n",
    "}\n",
    "\n",
    "ZIP_FEATURES = pd.read_csv('/Users/chrissoria/Documents/Research/BICS_Political_Polarization/data/ZIP_Features.csv',\n",
    "                          dtype=col_types)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {\n",
    "    'resp_zip': str,\n",
    "    'political_party': str\n",
    "}\n",
    "\n",
    "all_waves = df_combined_contacts\n",
    "\n",
    "all_waves = all_waves.rename(columns={\n",
    "    'resp_zip': 'ZIP'\n",
    "})\n",
    "\n",
    "all_waves['ZIP'] = all_waves['ZIP'].str.strip()\n",
    "\n",
    "columns_to_keep = ['ResponseId','StartDate','resp_yob','resp_sex','resp_hispanic','r_race','resp_nativity','ZIP',\n",
    "                  'resp_hhsize','r_working','resp_occupation','lefthome_num','num_cc_nonhh',\n",
    "                  'lefthome_where_1','lefthome_where_2','lefthome_where_3','lefthome_where_4','lefthome_where_10',\n",
    "                  'lefthome_where_8','lefthome_where_9','lefthome_where_5','lefthome_where_11','lefthome_where_6',\n",
    "                  'lefthome_where_7','inet_freq','socmedia_use','covid19_familiar','covid19_concern',\n",
    "                  'covid19_f2fchange','covid19_reduceOK','policy_sip','age','hhi','political_party','political_view',\n",
    "                  'industry','health_insurance','interview_date','wave','agecat','city','covid19_vax','covid19_whynot_vax',\n",
    "                  'Non_Weighted_Safety_Index','Weighted_Safety_Index','Norm_Masks_Used','educ_group','contact_reduction',\n",
    "                  'binary_concern','binary_concern_strong','resp_educ','resp_sex','Contacts','Non_Weighted_Safety_Index',\n",
    "                  'Weighted_Safety_Index','data_collected_dates']\n",
    "all_waves = all_waves[columns_to_keep]\n",
    "\n",
    "all_waves['political_party'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefe16d",
   "metadata": {},
   "source": [
    "How many people have a ZIP code? ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad242117",
   "metadata": {},
   "source": [
    "what percentage of these are 5 digit zip codes? A lot of these are four digit zip codes. There are so few of them it's not worth trying to figure out where they are (4 digits is enough to tell me something about their localility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6773d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BICS_ZIP_FEATURES = pd.merge(all_waves, ZIP_FEATURES, on = \"ZIP\", how = \"left\")\n",
    "BICS_ZIP_FEATURES = BICS_ZIP_FEATURES.drop_duplicates(subset='ResponseId', keep='first')\n",
    "BICS_ZIP_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be196fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BICS_ZIP_FEATURES['ZIP'].isna().sum())\n",
    "print(BICS_ZIP_FEATURES['ZIP'].isna().mean() *100)\n",
    "print((BICS_ZIP_FEATURES['ZIP'].apply(lambda x: len(str(x)) == 5).mean()) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28190e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BICS_ZIP_FEATURES['junk_zip'] = BICS_ZIP_FEATURES['ZIP'].apply(lambda x: 1 if len(str(x)) == 5 else 0)\n",
    "BICS_junk_zips = BICS_ZIP_FEATURES[BICS_ZIP_FEATURES['junk_zip'] == 0]\n",
    "BICS_junk_zips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c36d05",
   "metadata": {},
   "source": [
    "How many zip codes are being matched? \\\n",
    "currently I'm getting, 33 percent large percent missing. \\\n",
    "Fixed the issue by reading in ZIPS as string at the top now only .8788 don't match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc7f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BICS_ZIP_FEATURES['USPS_STATE'].isna().sum())\n",
    "print(BICS_ZIP_FEATURES['USPS_STATE'].isna().mean() *100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90792bc",
   "metadata": {},
   "source": [
    "what about school closures? \\\n",
    "About 9 percent of people have a missing school matched to their ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BICS_ZIP_FEATURES['earlyfall2020_school_closures'].isna().sum())\n",
    "print(BICS_ZIP_FEATURES['earlyfall2020_school_closures'].isna().mean() *100)\n",
    "\n",
    "print(BICS_ZIP_FEATURES['fall2020_school_closures'].isna().sum())\n",
    "print(BICS_ZIP_FEATURES['fall2020_school_closures'].isna().mean() *100)\n",
    "\n",
    "print(BICS_ZIP_FEATURES['spring2021_school_closures'].isna().sum())\n",
    "print(BICS_ZIP_FEATURES['spring2021_school_closures'].isna().mean() *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb680fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BICS_ZIP_FEATURES['political_party_to_CD'] = BICS_ZIP_FEATURES.apply(\n",
    "    lambda row: str(row['political_party']) + \" in \" + str(row['CONGRESSPERSON_PARTY']) + \" led CD\", axis=1\n",
    ")\n",
    "\n",
    "# Print the updated DataFrame to see if it matches\n",
    "print(BICS_ZIP_FEATURES[['political_party', 'CONGRESSPERSON_PARTY', 'political_party_to_CD']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BICS_ZIP_FEATURES['political_party'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BICS_ZIP_FEATURES['CONGRESSPERSON_PARTY'] = BICS_ZIP_FEATURES['CONGRESSPERSON_PARTY'].replace(\"Democratic\", \"Democrat\")\n",
    "BICS_ZIP_FEATURES['In_Opposing_Party_CD'] = ((BICS_ZIP_FEATURES['political_party'] != BICS_ZIP_FEATURES['CONGRESSPERSON_PARTY']) & ~pd.isna(BICS_ZIP_FEATURES['CONGRESSPERSON_PARTY'])).astype(int)\n",
    "BICS_ZIP_FEATURES['Independent'] = (BICS_ZIP_FEATURES['political_party'] == \"Independent\").astype(int)\n",
    "BICS_ZIP_FEATURES['Vaccinated'] = BICS_ZIP_FEATURES['covid19_vax'].apply(\n",
    "    lambda x: 1 if x == \"Yes, I have received at least one dose of a vaccine\" else\n",
    "              (0 if x == \"No, I have not\" else np.nan)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07fd8c7",
   "metadata": {},
   "source": [
    "Does it make sense to drop Independents? \\\n",
    "They're kind of in their own cateogory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835921a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BICS_ZIP_FEATURES['In_Opposing_Party_CD'].value_counts())\n",
    "print(BICS_ZIP_FEATURES['political_party'].value_counts())\n",
    "print(BICS_ZIP_FEATURES['CONGRESSPERSON_PARTY'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22112fc7",
   "metadata": {},
   "source": [
    "Below I convert the raw numbers to categories for Trump and Biden share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa630a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BICS_ZIP_FEATURES['Categorical_Trump_County_Share'] = BICS_ZIP_FEATURES['trump_percentage_won'].apply(lambda x:\n",
    "    \"Less Than a Third\" if x < 33.1 else\n",
    "    \"Less than Two Thirds Greater Than One Third\" if x < 66.1 else\n",
    "    \"Greater than or Equal to Two thirds\" if x >= 66.1 else pd.NA\n",
    ")\n",
    "\n",
    "BICS_ZIP_FEATURES['Categorical_Biden_County_Share'] = BICS_ZIP_FEATURES['biden_percentage_won'].apply(lambda x:\n",
    "    \"Less Than a Third\" if x < 33.1 else\n",
    "    \"Less than Two Thirds Greater Than One Third\" if x < 66.1 else\n",
    "    \"Greater than or Equal to Two thirds\" if x >= 66.1 else pd.NA\n",
    ")\n",
    "\n",
    "BICS_ZIP_FEATURES['Percentage_Trump_Greater_Than_Two_Thirds'] = BICS_ZIP_FEATURES['trump_percentage_won'].apply(lambda x:\n",
    "    1 if x >= 66.1 else\n",
    "    0 if x < 66.1 else pd.NA\n",
    ")\n",
    "\n",
    "BICS_ZIP_FEATURES['Percentage_Trump_Less_Than_Two_Thirds'] = BICS_ZIP_FEATURES['trump_percentage_won'].apply(lambda x:\n",
    "    1 if x < 66.1 else\n",
    "    0 if x <= 66.1 else pd.NA\n",
    ")\n",
    "\n",
    "BICS_ZIP_FEATURES['Percentage_Biden_Greater_Than_Two_Thirds'] = BICS_ZIP_FEATURES['biden_percentage_won'].apply(lambda x:\n",
    "    1 if x >= 66.1 else\n",
    "    0 if x < 66.1 else pd.NA\n",
    ")\n",
    "\n",
    "BICS_ZIP_FEATURES['Percentage_Biden_Less_Than_Two_Thirds'] = BICS_ZIP_FEATURES['biden_percentage_won'].apply(lambda x:\n",
    "    1 if x < 66.1 else\n",
    "    0 if x <= 66.1 else pd.NA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c585d28",
   "metadata": {},
   "source": [
    "Next, I use the same logic to create the CD categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a840243",
   "metadata": {},
   "outputs": [],
   "source": [
    "BICS_ZIP_FEATURES['Categorical_Repub_CD_County_Share'] = BICS_ZIP_FEATURES['CD_PERCENT_REPUBLICAN'].apply(lambda x:\n",
    "    \"Less Than a Quarter\" if x < .25 else\n",
    "    \"Two Quarters\" if x < .50 and x >.25 else\n",
    "    \"Three Quarters\" if x < .75 and x > .50 else\n",
    "    \"Greater than or Equal to Three Quarters\" if x >= .75 else pd.NA\n",
    ")\n",
    "\n",
    "BICS_ZIP_FEATURES['Categorical_Dem_CD_County_Share'] = BICS_ZIP_FEATURES['CD_PERCENT_DEMOCRAT'].apply(lambda x:\n",
    "    \"Less Than a Quarter\" if x < .25 else\n",
    "    \"Two Quarters\" if x < .50 else\n",
    "    \"Three Quarters\" if x < .75 else\n",
    "    \"Greater than or Equal to Three Quarters\" if x >= .75 else pd.NA\n",
    ")\n",
    "\n",
    "BICS_ZIP_FEATURES['Categorical_Repub_CD_County_Share'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa8f36",
   "metadata": {},
   "source": [
    "Next, let's add a variable that treats urban and rural as a binary. \\\n",
    "1 = Metro - Counties in metro areas of 1 million population or more \\\n",
    "2 = Metro - Counties in metro areas of 250,000 to 1 million population \\         \n",
    "3 = Metro - Counties in metro areas of fewer than 250,000 population \\\n",
    "4 = Nonmetro - Urban population of 20,000 or more, adjacent to a metro area \\\n",
    "5 = Nonmetro - Urban population of 20,000 or more, not adjacent to a metro area \\\n",
    "6 = Nonmetro - Urban population of 2,500 to 19,999, adjacent to a metro area \\\n",
    "7 = Nonmetro - Urban population of 2,500 to 19,999, not adjacent to a metro area \\\n",
    "8 = Nonmetro - Completely rural or less than 2,500 urban population, adjacent to a metro area \\\n",
    "9 = Nonmetro - Completely rural or less than 2,500 urban population, not adjacent to a metro area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1588897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BICS_ZIP_FEATURES['COUNT_RUCC_CAT'] = BICS_ZIP_FEATURES['COUNTY_RUCC_2013'].apply(\n",
    "    lambda x: \"Urban Metro\" if x in [1, 2] else\n",
    "              \"Urban Nonmetro\" if x in list(range(3, 8)) else \n",
    "              \"Rural\" if x in [8, 9] else np.nan\n",
    ")\n",
    "\n",
    "BICS_ZIP_FEATURES['COUNT_RUCC_BINARY'] = BICS_ZIP_FEATURES['COUNTY_RUCC_2013'].apply(\n",
    "    lambda x: \"Urban\" if x in list(range(1, 8)) else  \n",
    "              \"Rural\" if x in [8, 9] else np.nan\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9502c0",
   "metadata": {},
   "source": [
    "Dopping all non-national values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "BICS_ZIP_FEATURES['city'].value_counts()\n",
    "\n",
    "BICS_ZIP_FEATURES = BICS_ZIP_FEATURES[BICS_ZIP_FEATURES['city'] == \"National\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a20a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "BICS_ZIP_FEATURES.to_csv('data/BICS_ZIP_Features.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbde180",
   "metadata": {},
   "outputs": [],
   "source": [
    "BICS_ZIP_FEATURES.to_csv('data/Partisanship and Health Behavior/Data/BICS_ZIP_Features.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
